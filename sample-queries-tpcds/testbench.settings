set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;

set hive.execution.engine=spark;
set hive.merge.sparkfiles=false;

set spark.home=/opt/hive_vec_package/spark202_no_Phive;
--set spark.master=yarn-client;
set spark.master=spark://r73-master:7077;
set spark.eventLog.enabled=true;
set spark.eventLog.dir=hdfs:///user/spark/applicationHistory_spark202;
set spark.serializer=org.apache.spark.serializer.KryoSerializer;
set hive.spark.job.monitor.timeout=7200s;
set hive.spark.client.server.connect.timeout=3600s;
set hive.spark.client.connect.timeout=3600s;
set spark.network.timeout=3600s;

set spark.kryo.referenceTracking=false;
set spark.io.compression.codec=lzf;
set spark.executor.extraJavaOptions=-XX:+UseParallelOldGC -XX:ParallelGCThreads=8 -XX:NewRatio=1 -XX:SurvivorRatio=1;
set spark.memory.storageFraction=0.01;

set spark.driver.memory=10g;
set spark.executor.cores=7;
set spark.executor.memory=28g;
set spark.yarn.executor.memoryOverhead=7168;
set spark.submit.deployMode=client;

set spark.dynamicAllocation.enabled=true;
set spark.shuffle.service.enabled=true;
set spark.shuffle.service.port=7337;
set spark.authenticate=false;
set spark.dynamicAllocation.executorIdleTimeout=60;
set spark.dynamicAllocation.minExecutors=0;
set spark.dynamicAllocation.schedulerBacklogTimeout=1;
set spark.executor.extraClassPath=/opt/hive_vec_package/hive232/conf;
set spark.driver.extraClassPath=/opt/hive_vec_package/hive232/conf;
set spark.driver.extraLibraryPath=/opt/hive_vec_package/hadoop273/lib/native;
set spark.executor.extraLibraryPath=/opt/hive_vec_package/hadoop273/lib/native;
set spark.yarn.am.extraLibraryPath=/opt/hive_vec_package/hadoop273/lib/native;
--set spark.yarn.jars=local:///opt/hos/spark202_noPhive_parquet/jars;
set spark.shuffle.registration.timeout=1200000;

set hive.vectorized.execution.enabled=true;
set hive.vectorized.execution.reduce.enabled=true;
set hive.vectorized.execution.reduce.groupby.enabled=true;
